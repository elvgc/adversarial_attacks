# Adversarial Attacks in Text-based LLMs

This is the **official repository** for the paper *“Exfiltrating Secrets: Confidentiality Risks of Prompt Injection Attacks in Text-based Large Language Models”*.  
It contains the code, experiments, and resources used in the study of prompt injection attacks on large language models.  
The repository is made available to encourage transparency, reproducibility, and further academic research in the field of AI security.  

---

## Ethical Considerations and Notes on Responsible Use

In conducting this research, all prompt injection experiments were carried out solely for academic purposes, with the explicit aim of evaluating model robustness and raising awareness of potential security gaps. Although some tests were performed through public inference APIs (e.g., Hugging Face), no attempts were made to cause harm, disrupt services, or exfiltrate sensitive data beyond controlled canary tokens. 

Furthermore, the code developed for this study has been made openly available in a public GitHub repository to foster reproducibility and further research. However, we want to emphasize that this code must be used responsibly and exclusively for legitimate research or educational purposes, in line with ethical AI practices.  
